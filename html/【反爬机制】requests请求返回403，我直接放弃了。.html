<html>
 <head>
  <title>
   【反爬机制】requests请求返回403，我直接放弃了。
  </title>
  <link data-noprefix="" href="prism.css" rel="stylesheet">
  <style>
   a{
    color: black;
    text-decoration: none;
}
a:hover{
    color: blue;
}
ul{
    list-style: none;
}
#catalogue {
    top: 10px;
    right: 10px;
    z-index: 1;
    padding: 5px 10px;
    position: absolute;
    border: 1px solid blue;
    background-color: white;
}
  </style>
 </head>
 <body>
  <h1>
   <a>
    【反爬机制】requests请求返回403，我直接放弃了。
   </a>
  </h1>
  <ul id="catalogue">
   <li title="返回目录">
    <a href="/html">
     返回目录
    </a>
   </li>
   <li title="1、从用户请求的Headers反爬虫。">
    <a href="#user-content-1从用户请求的headers反爬虫">
     1、从用户请求的Headers反爬虫。
    </a>
   </li>
   <li title="2、基于用户行为反爬虫">
    <a href="#user-content-2基于用户行为反爬虫">
     2、基于用户行为反爬虫
    </a>
   </li>
   <li title="3、动态页面的反爬虫">
    <a href="#user-content-3动态页面的反爬虫">
     3、动态页面的反爬虫
    </a>
   </li>
  </ul>
  <p>
   大家好，我自以为自学了一点爬虫就了不起了，没想到它给了我当头一棒。
  </p>
  <p>
   <img alt="" src="https://img-blog.csdnimg.cn/91ff91835ae74841bd20ced21ab41d68.png" style="max-width: 100%;">
  </p>
  <p>
   <em>
    **
    <strong>
     403 （禁止） 服务器拒绝请求
    </strong>
    。**
   </em>
  </p>
  <p>
   403状态码可以简单的理解为没有权限访问此站。该状态表示服务器理解了本次请求但是拒绝执行该任务.
  </p>
  <p>
   这就是绝对多数网站的
   <strong>
    反爬机制
   </strong>
   。那我们简单了解一下。
  </p>
  <p>
   一般网站从三个方面反爬虫：
  </p>
  <p>
   <a href="#1%E3%80%81%E4%BB%8E%E7%94%A8%E6%88%B7%E8%AF%B7%E6%B1%82%E7%9A%84Headers%E5%8F%8D%E7%88%AC%E8%99%AB%E3%80%82">
    1、从用户请求的Headers反爬虫。
   </a>
  </p>
  <p>
   <a href="#2%E3%80%81%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%8F%8D%E7%88%AC%E8%99%AB">
    2、基于用户行为反爬虫
   </a>
  </p>
  <p>
   <a href="#3%E3%80%81%E5%8A%A8%E6%80%81%E9%A1%B5%E9%9D%A2%E7%9A%84%E5%8F%8D%E7%88%AC%E8%99%AB">
    3、动态页面的反爬虫
   </a>
  </p>
  <hr>
  <p>
   前两种比较容易遇到，大多数网站都从这些角度来反爬虫。第三种一些应用ajax的网站会采用，这样增大了爬取的难度，防止静态爬虫使用ajax技术动态加载页面。
  </p>
  <h3>
   <a aria-hidden="true" class="anchor" href="#1从用户请求的headers反爬虫" id="user-content-1从用户请求的headers反爬虫">
    <span aria-hidden="true" class="octicon octicon-link">
    </span>
   </a>
   1、从用户请求的Headers反爬虫。
  </h3>
  <blockquote>
   <p>
    这种是最常见的反爬机制，在访问某些网站的时候，网站通常会用判断访问是否带有头文件来鉴别该访问是否为爬虫，用来作为反爬取的一种策略。
   </p>
   <p>
    那我们就需要伪装headers。很多网站都会对Headers的User-Agent进行检测。
   </p>
   <p>
    如果遇到了这类反爬虫机制，可以直接在爬虫中添加Headers，将浏览器的User-Agent复制到爬虫的Headers中。
   </p>
  </blockquote>
  <p>
   还有就是检测防盗链referer
  </p>
  <blockquote>
   <p>
    还有一部分网站会对Referer进行检测，一些资源网站的防盗链就是检测Referer。将Referer值修改为目标网站域名。往往容易被忽略，通过对请求的抓包分析，确定referer，在程序中模拟访问请求头中添加。
   </p>
  </blockquote>
  <p>
   对于检测Headers的反爬虫，在爬虫中修改或者添加Headers就能很好的绕过。
  </p>
  <h3>
   <a aria-hidden="true" class="anchor" href="#2基于用户行为反爬虫" id="user-content-2基于用户行为反爬虫">
    <span aria-hidden="true" class="octicon octicon-link">
    </span>
   </a>
   2、基于用户行为反爬虫
  </h3>
  <p>
   还有一部分网站是通过检测用户行为，例如同一IP短时间内多次访问同一页面，或者同一账户短时间内多次进行相同操作。这样的话，可能会被封ip，我们就不能继续获取数据了。
  </p>
  <p>
   这种反爬，需要有足够多的ip来应对。
  </p>
  <p>
   大多数网站都是前一种情况，对于这种情况，使用IP代理就可以解决。有的还可以在每次请求后随机间隔几秒再进行下一次请求。
  </p>
  <h3>
   <a aria-hidden="true" class="anchor" href="#3动态页面的反爬虫" id="user-content-3动态页面的反爬虫">
    <span aria-hidden="true" class="octicon octicon-link">
    </span>
   </a>
   3、动态页面的反爬虫
  </h3>
  <p>
   上述的几种情况大多都是出现在静态页面，还有一部分网站，我们需要爬取的数据是通过ajax请求得到，或者通过Java生成的。
  </p>
  <blockquote>
   <p>
    方法就是抓包分析突破异步加载，selenium自动化测试工具。
   </p>
  </blockquote>
  <p>
   这个难度就比较多，对新手不太友好，我还有待提高，努力学会这个。
  </p>
  <p>
   <em>
    <strong>
     最后嘱咐大家一句，爬虫世界确实很有意思，技术是无罪的，学习是可以的，但还是实际操作就要适可而止了，不要触碰到法律的边界线
    </strong>
   </em>
  </p>
  <script src="prism.js">
  </script>
  <script>
   window.onscroll = function() {
    document.getElementById('catalogue').style.top = window.pageYOffset + 'px';
}
  </script>
 </body>
</html>