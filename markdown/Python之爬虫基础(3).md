# ![522bfcbd8cd948878af907787a3297b5.jpg](https://img-blog.csdnimg.cn/522bfcbd8cd948878af907787a3297b5.jpg)

# 爬虫基础

今天我们将介绍爬虫的基本原理和代理的基本原理相关知识。

**目录**

[爬虫基础](#%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80)

[1.3爬虫的基本原理](#1.3%E7%88%AC%E8%99%AB%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86)

[1.3.1爬虫概述](#1.3.1%E7%88%AC%E8%99%AB%E6%A6%82%E8%BF%B0)

[1.3.2能抓怎样的数据](#1.3.2%E8%83%BD%E6%8A%93%E6%80%8E%E6%A0%B7%E7%9A%84%E6%95%B0%E6%8D%AE)

[1.3.3 JavaScript 渲染页面](<#1.3.3 JavaScript 渲染页面>)

[1.4代理的基本原理](#1.4%E4%BB%A3%E7%90%86%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86)

[1.4.1基本原理](#1.4.1%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86)

[1.4.2代理的作用](#1.4.2%E4%BB%A3%E7%90%86%E7%9A%84%E4%BD%9C%E7%94%A8)

[1.4.3爬虫代理](#1.4.3%E7%88%AC%E8%99%AB%E4%BB%A3%E7%90%86)

[1.4.4代理分类](#1.4.4%E4%BB%A3%E7%90%86%E5%88%86%E7%B1%BB)

[1.4.5常见代理设置](#1.4.5%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE)

---

## 1.3爬虫的基本原理

我们可以把互联网比作一张大网，而爬虫（即网络爬虫）便是在网上爬行的蜘蛛。把网的节点比作一个个网页，爬虫爬到这就相当于访问了该页面，获取了其信息。可以把节点间的连线比作网页与网页之间的链接关系，这样蜘蛛通过一个节点后，可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，网站的数据就可以被抓取下来了。

### 1.3.1爬虫概述

简单来说，爬虫就是获取网页并提取和保存信息的自动化程序，下面概要介绍一下。  
**1．获取网页**  
爬虫首先要做的工作就是获取网页，这里就是获取网页的源代码。源代码里包含了网页的部分有用信息，所以只要把源代码获取下来，就可以从中提取想要的信息了。

前面讲了请求和响应的概念，向网站的服务器发送一个请求，返回的响应体便是网页源代码。所以，最关键的部分就是构造一个请求并发送给服务器，然后接收到响应并将其解析出来，那么这个流程怎样实现呢？总不能手工去截取网页源码吧？  
不用担心， Python 提供了许多库来帮助我们实现这个操作，如 urllib 、 requests 等。我们可以用这些库来帮助我们实现 HTTP 请求操作，请求和响应都可以用类库提供的数据结构来表示，得到响应之后只需要解析数据结构中的 Body 部分即可，即得到网页的源代码，这样我们可以用程序来实现获取网页的过程了。

**2.提取信息**

获取网页源代码后，接下来就是分析网页源代码，从中提取我们想要的数据。首先，最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错。  
另外，由于网页的结构有一定的规则，所以还有一些根据网页节点属性、 CSS 选择器或 XPath 来提取网页信息的库，如 Beautiful Soup 、 pyquery 、 Ixml 等。使用这些库，我们可以高效快速地从中提取网页信息，如节点的属性、文本值等。  
提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得条理清晰，以便我们后续处理和分析数据。

**3．保存数据**

提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为 TXT 文本或 JSON 文本，也可以保存到数据库，如 MySQL 和 MongoDB 等，也可保存至远程服务器，如借助 SFTP 进行操作等。

**4．自动化程序**

说到自动化程序，意思是说爬虫可以代替人来完成这些操作。首先，我们手工当然可以提取这些信息，但是当量特别大或者想快速获取大量数据的话，肯定还是要借助程序。爬虫就是代替我们来完成这份爬取工作的自动化程序，它可以在抓取过程中进行各种异常处理、错误重试等操作，确保爬取持续高效地运行。

### 1.3.2能抓怎样的数据

在网页中我们能看到各种各样的信息，最常见的便是常规网页，它们对应着 HTML 代码，而最常抓取的便是 HTM 源代码。

另外，可能有些网页返回的不是 HTML 代码，而是一个 JSON 字符串（其中 API 接口大多采用这样的形式），这种格式的数据方便传输和解析，它们同样可以抓取，而且数据提取更加方便。  
此外，我们还可以看到各种二进制数据，如图片、视频和音频等。利用爬虫，我们可以将这些二进制数据抓取下来，然后保存成对应的文件名。  
另外，还可以看到各种扩展名的文件，如 CSs 、 JavaScript 和配置文件等，这些其实也是最普通的文件，只要在浏览器里面可以访问到，就可以将其抓取下来。  
上述内容其实都对应各自的 URL ，是基于 HTTP 或 HTTPS 协议的，只要是这种数据，爬虫都可以抓取。

### 1.3.3 JavaScript 渲染页面

有时候，我们在用 urlib 或 requests 抓取网页时，得到的源代码实际和浏览器中看到的不一样。这是一个非常常见的问题。现在网页越来越多地采用 Ajax 、前端模块化工具来构建，整个网页可能都是由 JavaScript 渲染出来的，也就是说原始的 HTML 代码就是一个空壳，例如：

> \< \!DOCTYPE html>  
> \< html >  
> \< head >  
> \< meta charset =" UTF \-8">  
> \< title > This is a Demo \</ title >\</ head >  
> \< body >  
> \< div id =" container ">  
> \</ div >  
> \</ body >  
> \< Script src =" app . js ">\</ script >  
> \</ html >

body 节点里面只有一个 id 为 container 的节点，但是需要注意在 body 节点后引人了 appjs ，它便负责整个网站的渲染。  
在浏览器中打开这个页面时，首先会加载这个 HTML 内容，接着浏览器会发现其中引人了一个 app . js 文件，然后便会接着去请求这个文件，获取到该文件后，便会执行其中的 JavaScript 代码，而JavaScript 则会改变 HTML 中的节点，向其添加内容，最后得到完整的页面。

但是在用 urlib 或 requests 等库请求当前页面时，我们得到的只是这个 HTML 代码，它不会帮助我们去继续加载这个 JavaScript 文件，这样也就看不到浏览器中的内容了。  
这也解释了为什么有时我们得到的源代码和浏览器中看到的不一样。  
因此，使用基本 HTTP 请求库得到的源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，我们可以分析其后台 Ajax 接口，也可使用 Selenium 、 Splash 这样的库来实现模拟 JavaScript 渲染。

后面，我们会详细介绍如何采集 JavaScript 渲染的网页。  
这里介绍了爬虫的一些基本原理，这可以帮助我们在后面编写爬虫时更加得心应手。

## 1.4代理的基本原理

我们在做爬虫的过程中经常会遇到这样的情况，最初爬虫正常运行，正常抓取数据，一切看起来都是那么美好，然而一杯茶的功夫可能就会出现错误，比如403 Forbidden ，这时候打开网页一看，可能会看到“您的 IP 访问频率太高”这样的提示。出现这种现象的原因是网站采取了一些反爬虫措施。比如，服务器会检测某个 IP 在单位时间内的请求次数，如果超过了这个阈值，就会直接拒绝服务，返回一些错误信息，这种情况可以称为封 IP 。

既然服务器检测的是某个 IP 单位时间的请求次数，那么借助某种方式来伪装我们的 IP ，让服务器识别不出是由我们本机发起的请求，不就可以成功防止封 IP 了吗？  
一种有效的方式就是使用代理，后面会详细说明代理的用法。在这之前，需要先了解下代理的基本原理，它是怎样实现 IP 伪装的呢？

### 1.4.1基本原理

代理实际上指的就是代理服务器，英文叫作 proxy server ，它的功能是代理网络用户去取得网络信息。形象地说，它是网络信息的中转站。在我们正常请求一个网站时，是发送了请求给 Web 服务器， Web 服务器把响应传回给我们。如果设置了代理服务器，实际上就是在本机和服务器之间搭建了一个桥，此时本机不是直接向 Web 服务器发起请求，而是向代理服务器发出请求，请求会发送给代理服务器，然后由代理服务器再发送给 Web 服务器，接着由代理服务器再把 Web 服务器返回的响应转发给本机。这样我们同样可以正常访问网页，但这个过程中 Web 服务器识别出的真实 IP 就不再是我们本机的 IP 了，就成功实现了 IP 伪装，这就是代理的基本原理。

### 1.4.2代理的作用

> - 那么，代理有什么作用呢？我们可以简单列举如下。
> - 突破自身 IP 访问限制，访问一些平时不能访问的站点。将访问一些单位或团体内部资源：比如使用教育网内地址段免费代理服务器，就可以用于对教育网开放的各类 FTP 下载上传，以及各类资料查询共享等服务。
> - 提高访问速度：通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将其保存到缓冲区中，当其他用户再访问相同的信息时，则直接由缓冲区中取出信息，传给用户，以提高访问速度。
> - 隐藏真实 IP ：上网者也可以通过这种方法隐藏自己的 IP ，免受攻击。对于爬虫来说，我们用代理就是为了隐藏自身 IP ，防止自身的 IP 被封锁。

### 1.4.3爬虫代理

对于爬虫来说，由于爬虫爬取速度过快，在爬取过程中可能遇到同一个 IP 访问过于频繁的问题，此时网站就会让我们输人验证码登录或者直接封锁 P ，这样会给爬取带来极大的不便。  
使用代理隐藏真实的 IP ，让服务器误以为是代理服务器在请求自己。这样在爬取过程中通过不断更换代理，就不会被封锁，可以达到很好的爬取效果。

### 1.4.4代理分类

代理分类时，既可以根据协议区分，也可以根据其匿名程度区分。

**1.根据协议区分**

根据代理的协议，代理可以分为如下类别。

> - HTTP 代理服务器：主要用于访问网页，一般有内容过滤和缓存功能，端口一般为80、8080、3128等。
> - SSL / TLS 代理：主要用于访问加密网站，一般有 SSL 或 TLS 加密功能（最高支持128位加密强度），端口一般为443。
> - RTSP 代理：主要用于访问 Real 流媒体服务器，一般有缓存功能，端口一般为554。
> - Telnet 代理：主要用于 telnet 远程控制黑客人侵计算机时常用于隐藏身份），端口一般为23。
> - POP3/SMTP代理：主要用于POP3/SMTP方式收发邮件，一般有缓存功能，端口一般为110/25。
> - SOCKS 代理：只是单纯传递数据包，不关心具体协议和用法，所以速度快很多，一般有缓存功能，端口一般为1080。 SOCKS 代理协议又分为SOCKS4和SOCKS5，前者只支持 TCP ,而后者支持 TCP 和 UDP ，还支持各种身份验证机制、服务器端域名解析等。简单来说，SOCKS4能做到的SOCKS5都可以做到，但SOCKS5能做到的SOCKS4不一定能做到。

**2．根据匿名程度区分**

根据代理的匿名程度，代理可以分为如下类别

> - 高度匿名代理：会将数据包原封不动地转发，在服务端看来就好像真的是一个普通客户端在访问，而记录的 IP 是代理服务器的 P 。
> - 普通匿名代理：会在数据包上做一些改动，服务端上有可能发现这是个代理服务器，也有一定几率追查到客户端的真实 P 。代理服务器通常会加人的 HTTP 头有 HTTP VIA 和 HTTPX \_ FORWARDED \_ FOR 。
> - 透明代理：不但改动了数据包，还会告诉服务器客户端的真实 IP 。这种代理除了能用缓存技术提高浏览速度，能用内容过滤提高安全性之外，并无其他显著作用，最常见的例子是内网中的硬件防火墙
> - 间谍代理：指组织或个人创建的用于记录用户传输的数据，然后进行研究、监控等目的的代理服务器。

### 1.4.5常见代理设置

> - 使用网上的免费代理：最好使用高匿代理，另外可用的代理不多，需要在使用前筛选一下可用代理，也可以进一步维护一个代理池。
> - 使用付费代理服务：互联网上存在许多代理商，可以付费使用，质量比免费代理好很多。
> - ADSL 拨号：拨一次号换一次 IP ，稳定性高，也是一种比较有效的解决方案。

---

> 今天就简单分享到这里。