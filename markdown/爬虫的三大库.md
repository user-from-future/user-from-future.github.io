大家好，我是爱吃饼干的小白鼠。大家安装完python的requests，beautifulsoup ，lxml三个库，之后，我们就来说说怎么使用吧。然后我教大家一个简单的爬虫程序。

# requests库

requests库的作用就是请求网站获取网页数据的，让我们从最简单的实例说起，requests库的使用方法。

```python
import requests

url='http://www.ceweekly.cn/2022/1017/397973.shtml'
headers={
    'user-agent':' Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
    }
#发送请求
response=requests.get(url=url)
#输出
print(response)
print(response.text)
```

程序实现结果如下，\<response \[200\]>表示请求网页成功，若为404，400，则请求失败。

![](https://img-blog.csdnimg.cn/0adb0da9d1794854a258daeabfd4e0e6.png)

上面的程序我们加入了请求头，这样可以更好的抓取数据。

请求头如下：

```python
headers={
    'user-agent':' Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
    }
```

requests库有除了上述的get方法外，还有post方法等。post大多数是需要登录的，不管，get可以爬取大部分网站了。

requests库运行 **常见报错** 如下

ConnectionError:网路问题，拒绝连接

HTTPError:response返回了不成功的状态码

Timout:请求超时

TooManyRedirects:请求超过了设定的次数，一般来说，这样是为了反爬而设置的。

# BeautifulSoup库

BeautifulSoup库是一个非常流行的模块，它的作用是用来解析requests库请求的网页，并把网页源代码解析为soup文档，以便过滤数据。

```python
import requests
from bs4 import BeautifulSoup
url='http://www.ceweekly.cn/2022/1017/397973.shtml'
headers={
    'user-agent':' Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
    }
#发送请求
response=requests.get(url=url)
#输出
soup=BeautifulSoup(response.text,'html.parser')
print(soup.prettify())
```

下面是程序实现结果

![](https://img-blog.csdnimg.cn/c092363d07694117b936e0b55639f60d.png)

虽然看上去和我们第一个差不多，但是通过BeautifulSoup库解析得到的Soup文档是按照标准缩进格式的结构输出的，为后面的数据处理做准备。

BeautifulSoup库除了可以使用文中的html.parser解析器，还可以支持第三方的解析器，比如讲lxml，xml，html5lib解析器。

不管用哪种解析器解析后的soup文档，可以用find\(\),fing\_all\(\)方法定位我们需要的元素，本文篇幅有限，就不说他们的使用方法，我放一个例子吧。

```python
soup.find_all('div',"item")##查找div标签，class="item"

soup.find_all('div',class="item")

soup.find_all('div',attrs={"class":"item"})#attrs参数定义一个字典参数来搜索包含特殊属性的tag
```

还有一个selector（）方法，这个就更简单了，

```python
soup.selector(div.item>a>h1)#在浏览器开发者模式里右击选择copy，会有一个copy selector选项，即可得到
```

# Lxml库

Lxml库比BeautifulSoup库解析速度更快，我这里放一个案例，就不过多解释。

```python
import requests
import lxml
from lxml import etree

url='http://www.ceweekly.cn/2022/1017/397973.shtml'
headers={
    'user-agent':' Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
    }
#发送请求
response=requests.get(url=url)
#输出
print(response)
```

今天就说到这里，我们下一期见。