大家好，我自以为自学了一点爬虫就了不起了，没想到它给了我当头一棒。

![](https://img-blog.csdnimg.cn/91ff91835ae74841bd20ced21ab41d68.png)

_****403 （禁止） 服务器拒绝请求** 。**_

403状态码可以简单的理解为没有权限访问此站。该状态表示服务器理解了本次请求但是拒绝执行该任务.

这就是绝对多数网站的 **反爬机制** 。那我们简单了解一下。

一般网站从三个方面反爬虫：

[1、从用户请求的Headers反爬虫。](#1%E3%80%81%E4%BB%8E%E7%94%A8%E6%88%B7%E8%AF%B7%E6%B1%82%E7%9A%84Headers%E5%8F%8D%E7%88%AC%E8%99%AB%E3%80%82)

[2、基于用户行为反爬虫](#2%E3%80%81%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%8F%8D%E7%88%AC%E8%99%AB)

[3、动态页面的反爬虫](#3%E3%80%81%E5%8A%A8%E6%80%81%E9%A1%B5%E9%9D%A2%E7%9A%84%E5%8F%8D%E7%88%AC%E8%99%AB)

---

前两种比较容易遇到，大多数网站都从这些角度来反爬虫。第三种一些应用ajax的网站会采用，这样增大了爬取的难度，防止静态爬虫使用ajax技术动态加载页面。

### 1、从用户请求的Headers反爬虫。

> 这种是最常见的反爬机制，在访问某些网站的时候，网站通常会用判断访问是否带有头文件来鉴别该访问是否为爬虫，用来作为反爬取的一种策略。
> 
> 那我们就需要伪装headers。很多网站都会对Headers的User-Agent进行检测。
> 
> 如果遇到了这类反爬虫机制，可以直接在爬虫中添加Headers，将浏览器的User-Agent复制到爬虫的Headers中。

还有就是检测防盗链referer

> 还有一部分网站会对Referer进行检测，一些资源网站的防盗链就是检测Referer。将Referer值修改为目标网站域名。往往容易被忽略，通过对请求的抓包分析，确定referer，在程序中模拟访问请求头中添加。

对于检测Headers的反爬虫，在爬虫中修改或者添加Headers就能很好的绕过。

### 2、基于用户行为反爬虫

还有一部分网站是通过检测用户行为，例如同一IP短时间内多次访问同一页面，或者同一账户短时间内多次进行相同操作。这样的话，可能会被封ip，我们就不能继续获取数据了。

这种反爬，需要有足够多的ip来应对。

大多数网站都是前一种情况，对于这种情况，使用IP代理就可以解决。有的还可以在每次请求后随机间隔几秒再进行下一次请求。

### 3、动态页面的反爬虫

上述的几种情况大多都是出现在静态页面，还有一部分网站，我们需要爬取的数据是通过ajax请求得到，或者通过Java生成的。

> 方法就是抓包分析突破异步加载，selenium自动化测试工具。

这个难度就比较多，对新手不太友好，我还有待提高，努力学会这个。

_**最后嘱咐大家一句，爬虫世界确实很有意思，技术是无罪的，学习是可以的，但还是实际操作就要适可而止了，不要触碰到法律的边界线**_